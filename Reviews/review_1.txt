Report on Fast Automatic Bayesian Cubature Using Lattice Sampling by Jagadeeswaran and Hickernell

The paper is an interesting read on Bayesian cubature from the probabilistic
numerics field, on which I am not an expert.
The authors show how the standard algorithm works and show that under certain
assumptions it has a cost of O(n^3) in the number of samples. These assumptions
are not well explained in the paper. See further.
They then show how, using the structure of lattice points and a matching form
of the covariance kernel in their Gaussian process, this cost can be severely
reduced to O(n \log(n)).
The paper is well written, although the authors are sometimes a bit quick to
draw conclusions.
The speedup of their algorithm is in the linear algebra part of the
calculations. But the derivation in Sections 4.3 and 4.4 can be obtained much
faster by realising that their definition of the covariance matrix,
   C_{i,j} = K( x_i - x_j mod 1 ) = K( \vec{h} \phi(i-1) - \vec{h} \phi(j-1) mod 1 )
           = g_{\vec{h}}( \pi_{2^m}(i-1) - \pi_{2^m}(j-1) mod 2^m )
           = g_{\vec{h},\pi_{2^m}}( (i-1) - (j-1) mod 2^m )  ,   i,j=1,...,2^m ,
is a permutation of a circulant matrix from which all required properties follow.
(In here \phi is the radical inverse function and \pi_{2^m}(i-1) = 2^m\phi(i-1),
the ''integer version'' of the radical inverse function for the full initial
sequence of 2^m points). The current Sections 4.3 and 4.4 are therefore not
needed. See further.
A second contribution is made in section 4.5 titled 'Overcoming cancellation
error', but here it is not so clear to me if what is proposed will really help.
See further.

I think the authors should carefully revise the manuscript and believe it is
worthy of publication afterwards.

Now follow the more detailed comments:

- Abstract
-- The second sentence doesn't make sense to me.
-- In the third sentence I would replace 'postulates' with 'considers'.

- Section 1
-- The authors write 'We construct a reliable stopping criterion' but it is
unclear what they mean with 'reliable'.
-- On page 2 they write 'If function evaluation is expensive, then $(f) might
be similar in magnitude to \log(n).': this makes no sense as evaluating f
depends on the dimension d. So this depends on how big n is and if we want more
accuracy then n becomes bigger.

- Section 2
-- 'f is drawn from a Guaussian process': it should be explained what this
means, e.g., does it mean f is the Gaussian function with unknown mean and
covariance, or does it behave like this in expectation, or ... This sounds like
a very strong assumption.
-- It should maybe be mentioned here that there are hyper distributions for the
hyper parameters.
-- It is never said how the x_i are sampled. Does (5a) hold for any
distribution of samples x_i?
-- In this section there are different quantities being defined: the matrix
C_{\vec{\theta}} which involves evaluating the covariance function, c_0 which
involves integrating the covariance function and \vec{c} which involves
calculating integrals of the covariance function with one leg fixed to a sample
point. Only at the end the authors say that they assume these things to be
cheap. I think this should be spelled out immediately.
-- It think is not a good idea to leave out the dependence on \vec{\theta} for
C. E.g. in (11) the \vec{\theta} is not visible at all in the expression to be
minimized for it. The C without subscript also clashes that of lemma 1.
-- The lead in to lemma 1 makes it feel like this is a field specific lemma but
in fact it is just the well known conditional gaussian. Please write something
like 'We need the following lemma on conditional normal distribution ...'.
-- Do not use C in this lemma.
-- The other derivations in the paper are all made in prose. It would be better
to formally state some of them. E.g., the results on the top of page 3.
-- From (6) to (7) the notation is different: c^T C^{-1} 1 vs 1^T C^{-1} c,
this might be corrected.

- Section 2.2.1
-- It would help to see the dependence on \vec{\theta} in (9), (10) and (11).
-- It looks like (11) is independent of m_MLE and s^2_MLE, how does it pop up?
Otherwise we could just first optimize for \vec{\theta}, then for m and then
for s^2.

- Section 2.2.2
-- I cannot follow the derivation.
-- At the end of the section the authors say that this is not a good method.
Why then list it? Maybe the final paragraph needs to be rewritten.

- Section 2.3
-- The authors stress here that the integrand must be a 'typical' draw from the
assumed Gaussian process and not an outlier. That sounds scary.

- Section 2.4
-- Please use d' for the dimension of the original integral in (22) and
immediately say that the dimension for the computation is d = d' - 1, 'as will
now be explained...'. So put d' after the display (22) and in the display (23)
and update the formula for \mu at the bottom of the page and the display on the
next page. After these changes you can keep thee 'd=2' in the caption of Fig 2.

- Section 3
-- Maybe write 'previous section' instead of 'last section'.
-- Maybe write 'covariance kernel' instead of just 'kernel'. I think the word
kernel hasn't been used up till this point.

- Section 3.1
-- Use C_{\vec{\theta}} in (27) for clarity.

- Section 3.3
-- Elaborate on what is meant with the second sentence.

- Section 4.2, 4.3 and 4.4
-- The definition (35) makes it that the covariance matrix C is a permutation
of a circulant matrix. Repeating the formula from above:
   C_{i,j} = K( x_i - x_j mod 1 ) = K( \vec{h} \phi(i-1) - \vec{h} \phi(j-1) mod 1 )
           = g_{\vec{h}}( \pi_{2^m}(i-1) - \pi_{2^m}(j-1) mod 2^m )
           = g_{\vec{h},\pi_{2^m}}( (i-1) - (j-1) mod 2^m )  ,   i,j=1,...,2^m ,
herein is \phi the radical inverse function (in base 2 in the current paper)
and the function \pi_{2^m}(i-1) = 2^m \phi(i-1).
This function \pi_{2^m} is a permutation on {0, 1, ..., 2^m-1}, undoing this
permutation on the rows and columns of the matrix C one obtains a circulant
matrix:
  P_{2^m}^T C P_{2^m}
and it is well known that a circulant matrix can be factorized by FFT.
The authors use similar arguments but need 2 pages of derivations.
In fact looking at (37) they should have realized this is a permutation of the
discrete Fourier matrix. I do not think we need to be convinced that there
exist fast algorithms for multiplication with the Fourier matrices. In fact
this makes that their algorithm can just as well be used for radical inverse
functions in a different base and probably also for non-power n.
The authors do some Cooley-Tukey derivations in Section 4.4 which is not needed
as soon as one realizes V is a permuted Fourier transform. The splitting which
is performed on the vectors b follows from the property of \pi_{2^m}(i-1) = 2^m
\phi(i-1) which first gives all even values and then all odd values, i.e.,
Cooley-Tukey.

- Section 4.5
-- The authors present an alternative form of calculating the matrix C for
covariance functions of the particular product form in Section 4.2 in which
they define a matrix Ccirc = C - 1.
The covariance function is given by
  C(t, x) = \prod_{j=1}^d (1 + g_j(t_j, x_j))
and they argue that if the g_j(t_j, x_j) are small then subtracting 1 at the
end results in cancellation error. I think they are right that this is a good
way of calculating Ccirc withouth doing the -1 at the end.
However, it is not clear whether this will actually help. The reason they look
at Ccirc is because they want to have the eigenvalue \lambda_1 (which is
associated with the all ones eigenvector). Now they calculate \lambdacirc_1
which equals \lambda_1 - n, which means \lambdacirc_1 is small under their
assumption that n/\lambda_1 is close to 1. It is not clear to me if operating
the FFT on the first (permuted) column of Ccirc in this form is not going to
increase the error on \lambdacirc_1, so putting the numerical error elsewhere.
- Please delete the word 'periodic' in proposition 1, this is covered by the
shift-invariantness.

- Section 5.1
-- The Baker's transform is introduced first and then the authors talk about
more general transforms without saying so. For the Baker's transform the
substitution written with \Psi' is undefined. This paragraph should be
restructured.

